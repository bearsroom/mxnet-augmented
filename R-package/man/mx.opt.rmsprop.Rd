% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizer.R
\name{mx.opt.rmsprop}
\alias{mx.opt.rmsprop}
\title{Create an RMSProp optimizer with respective parameters.
Reference: Tieleman T, Hinton G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude[J]. COURSERA: Neural Networks for Machine Learning, 2012, 4(2).
The code follows: http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.}
\usage{
mx.opt.rmsprop(learning.rate = 0.002, gamma1 = 0.95, gamma2 = 0.9,
  wd = 0, rescale.grad = 1, clip_gradient = NULL, lr_scheduler = NULL)
}
\arguments{
\item{learning.rate}{float, default=0.002
Step size.}

\item{gamma1}{float, default=0.95
decay factor of moving average for gradient, gradient^2.}

\item{wd}{float, default=0.0
L2 regularization coefficient add to all the weights.}

\item{rescale.grad}{float, default=1.0
rescaling factor of gradient.}

\item{clip_gradient}{float, optional
clip gradient in range [-clip_gradient, clip_gradient].}

\item{lr_scheduler}{function, optional
The learning rate scheduler.}

\item{gamm2}{float, default=0.9
"momentum" factor.}
}
\description{
Create an RMSProp optimizer with respective parameters.
Reference: Tieleman T, Hinton G. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude[J]. COURSERA: Neural Networks for Machine Learning, 2012, 4(2).
The code follows: http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.
}

