% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizer.R
\name{mx.opt.adam}
\alias{mx.opt.adam}
\title{Create an Adam optimizer with respective parameters.
Adam optimizer as described in [King2014].}
\usage{
mx.opt.adam(learning.rate = 0.001, beta1 = 0.9, beta2 = 0.999,
  epsilon = 1e-08, wd = 0, rescale.grad = 1, clip_gradient = NULL,
  lr_scheduler = NULL)
}
\arguments{
\item{learning.rate}{float, default=0.001
Step size.}

\item{beta1}{float, default=0.9
Exponential decay rate for the first moment estimates.}

\item{beta2}{float, default=0.999
Exponential decay rate for the second moment estimates.}

\item{epsilon}{float, default=1e-8}

\item{wd}{float, default=0.0
L2 regularization coefficient add to all the weights.}

\item{rescale.grad}{float, default=1.0
rescaling factor of gradient.}

\item{clip_gradient}{float, optional
clip gradient in range [-clip_gradient, clip_gradient].}

\item{lr_scheduler}{function, optional
The learning rate scheduler.}
}
\description{
[King2014] Diederik Kingma, Jimmy Ba,
Adam: A Method for Stochastic Optimization,
http://arxiv.org/abs/1412.6980
}

