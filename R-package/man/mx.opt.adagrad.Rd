% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimizer.R
\name{mx.opt.adagrad}
\alias{mx.opt.adagrad}
\title{Create an AdaGrad optimizer with respective parameters.
AdaGrad optimizer of Duchi et al., 2011,}
\usage{
mx.opt.adagrad(learning.rate = 0.05, epsilon = 1e-08, wd = 0,
  rescale.grad = 1, clip_gradient = NULL, lr_scheduler = NULL)
}
\arguments{
\item{learning.rate}{float, default=0.05
Step size.}

\item{epsilon}{float, default=1e-8}

\item{wd}{float, default=0.0
L2 regularization coefficient add to all the weights.}

\item{rescale.grad}{float, default=1.0
rescaling factor of gradient.}

\item{clip_gradient}{float, optional
clip gradient in range [-clip_gradient, clip_gradient].}

\item{lr_scheduler}{function, optional
The learning rate scheduler.}
}
\description{
This code follows the version in http://arxiv.org/pdf/1212.5701v1.pdf  Eq(5)
by Matthew D. Zeiler, 2012. AdaGrad will help the network to converge faster
in some cases.
}

